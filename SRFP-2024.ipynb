{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic EDA on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "adware_dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Android Malware/Dynamic/AndMal2020-Dynamic-BeforeAndAfterReboot/Adware_after_reboot_Cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "adware_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "adware_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "columns = list(adware_dataset.columns)\n",
    "\n",
    "print(columns)\n",
    "\n",
    "print(len(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "missing_values = adware_dataset.isnull().sum()\n",
    "\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = adware_dataset.select_dtypes(include=['object']).columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "adware_dataset = adware_dataset.drop('Hash', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category and Family\n",
    "\n",
    "## Adware: Software that automatically displays or downloads advertising material when a user is online.\n",
    "  1. dowgin:  Displays intrusive ads, often in the form of pop-ups or banners. It can also change the homepage of the browser and redirect searches.\n",
    "  2. adflex: Inserts advertisements into web pages and applications. It may collect user browsing data to target ads more effectively.\n",
    "  3. admogo: Known for delivering ads through notifications and embedding ads in various parts of the operating system or applications.\n",
    "  4. adviator: Utilizes aggressive advertising techniques, such as full-screen ads and ads in the notification area. It can also slow down the device performance.\n",
    "  5. adwo: Often comes bundled with other applications. It collects user information to deliver targeted advertisements, sometimes leading to privacy concerns.\n",
    "  6. airpush: One of the most widespread adware families, known for pushing ads through notifications. It can also place ads directly on the home screen or within other applications.\n",
    "  7. appad: Displays ads in various forms, including banners, interstitial ads, and pop-ups. It often operates in the background without the user's consent.\n",
    "  8. appsgeyser: Associated with a platform for creating custom apps, it often injects ads into the apps created using its service, which can be intrusive to users.\n",
    "  9. baiduprotect: Known for embedding ads in applications and redirecting users to sponsored content. It can also modify browser settings and collect user data.\n",
    "  10. batmobi: Delivers ads through various channels, including in-app ads and notifications. It may also track user behavior to serve targeted ads.\n",
    "  11. dianjin: Primarily focused on serving ads through mobile apps. It can be persistent and difficult to remove without uninstalling the associated apps.\n",
    "  12. dianle: Inserts ads into the mobile experience, often by modifying existing apps or services. It can cause performance issues due to its resource usage.\n",
    "  13. domob: An adware family that displays ads within apps and games. It collects user data to optimize ad delivery and increase click-through rates.\n",
    "  14. ewind: Known for its stealthy behavior, ewind often hides its presence while continuously serving ads and collecting user information.\n",
    "  15. feiwo: Serves ads in various formats, such as banners and pop-ups. It can also redirect web traffic to sponsored pages.\n",
    "  16. fictus: Embeds ads within apps and modifies user settings to ensure ads are frequently displayed. It may also gather user data for targeted advertising.\n",
    "  17. ganlet: Displays ads in intrusive ways, often disrupting normal app usage. It can be challenging to detect and remove due to its integration with legitimate apps.\n",
    "  18. adend: Primarily focused on mobile advertising, adend serves ads through various channels, including in-app ads and web redirects\n",
    "  19. gmobi: A mobile adware family that delivers ads through apps and notifications. It can collect user data for advertising purposes\n",
    "  20. hiddenad: Operates by hiding its ad-serving capabilities within legitimate apps. It often displays ads in ways that are difficult for users to trace back to the source\n",
    "  21. hummingbad: A well-known adware family that not only serves ads but can also install additional malicious apps and root the device for deeper access.\n",
    "  22. igexin: Serves ads through mobile apps and collects extensive user data. It has been known to be used for tracking and profiling users\n",
    "  23. inmobi: A large ad network that can be used in apps to display ads. Some implementations have been criticized for being overly intrusive\n",
    "  24. inoco: Delivers ads in various formats and collects user data for targeting purposes. It can be particularly aggressive in its advertising methods\n",
    "  25. kalfere: Embeds ads within mobile apps and can display them in different formats. It may also alter browser settings to increase ad visibility\n",
    "  26. kuguo: Displays ads through apps and browsers, often using aggressive tactics to ensure high visibility and user engagement\n",
    "  27. leadbolt: Known for its comprehensive ad-serving platform, it can display ads in multiple formats, including banners, interstitials, and push notifications\n",
    "  28. mobclick: Serves ads through mobile apps, collecting user data to improve ad targeting and effectiveness\n",
    "  29. mobidash: Delivers ads through notifications and in-app placements. It can also track user behavior for targeted advertising\n",
    "  30. mobisec: Known for embedding ads within apps and modifying system settings to maintain ad visibility\n",
    "  31. mulad: Displays ads in various forms and can be persistent, making it difficult for users to avoid or remove\n",
    "  32. oimobi: Serves ads through mobile apps and collects user data to optimize ad delivery. It may also redirect web traffic to sponsored content\n",
    "  33. shedun: One of the most prolific adware families, known for aggressively embedding itself into apps and continuously serving intrusive ads\n",
    "  34. sprovider: Delivers ads through apps and browsers, often using techniques to ensure high visibility and engagement\n",
    "  35. viser: Inserts ads into various parts of the mobile experience, including apps and browsers. It may also alter user settings to maintain ad visibility\n",
    "  36. wooboo: Embeds ads within apps and can be challenging to remove due to its integration with legitimate functionalities\n",
    "  37. xynyin: Displays ads through apps and notifications, often collecting user data to improve ad targeting\n",
    "  38. zdtad: Serves ads through mobile apps and web browsers, using aggressive tactics to ensure user engagement\n",
    "  39. frupi: Delivers ads in various forms, including banners and pop-ups. It can also redirect web traffic to sponsored content\n",
    "  40. kyhub: Known for its stealthy ad-serving capabilities, often hiding its presence while continuously displaying ads\n",
    "  41. stopsms: Primarily serves ads through notifications and in-app placements, often disrupting normal usage\n",
    "  42. loki: Displays ads in intrusive ways, often modifying user settings to maintain ad visibility\n",
    "  43. kyview: Embeds ads within mobile apps, using various techniques to ensure high visibility and engagement\n",
    "  44. pandaad: Serves ads through apps and browsers, collecting user data for targeting purposes.\n",
    "  45. plague: Displays ads through mobile apps and web browsers, often using aggressive tactics to ensure high visibility\n",
    "  46. accutrack: Known for its ad-serving capabilities, embedding ads within apps and collecting user data for targeting\n",
    "  47. adcolony: Displays ads in various forms, including banners and interstitials. It can be persistent and difficult to remove\n",
    "  48. gexin: Delivers ads through mobile apps and collects user data to improve ad targeting and effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor: Malware that allows unauthorized access to a system by bypassing security mechanisms.\n",
    "\n",
    "1. kapuser:Known for creating hidden backdoors to allow remote access and control. It often modifies system settings and files to remain undetected.\n",
    "2. kmin: Employs techniques to establish a hidden communication channel between the infected device and the attacker. It can be used for data theft and further malware installation.\n",
    "3. fobus: Allows attackers to remotely control the infected device. It is often used to steal personal information or deploy additional malicious payloads.\n",
    "4. mobby: Provides remote access capabilities to the attacker, enabling them to execute commands, access files, and install other types of malware.\n",
    "5. hiddad: Not only acts as a backdoor but also hides its presence on the device. It often uses root exploits to gain deeper access and control.\n",
    "6. moavt: Known for its stealthy operations, allowing attackers to maintain long-term access to the infected device. It can also download and execute additional malicious code.\n",
    "7. androrat: A well-known remote administration tool for Android, giving attackers control over the device. It can capture keystrokes, take screenshots, and access camera/microphone.\n",
    "8. dendroid: Provides comprehensive remote control capabilities, including the ability to intercept messages, record audio, and track the device's location.\n",
    "9. levida: Focuses on creating persistent backdoors, allowing repeated unauthorized access even after attempts to remove it.\n",
    "10. pyls: Establishes a communication link with the attacker, often using encrypted channels to avoid detection. It can execute arbitrary commands and retrieve sensitive data.\n",
    "11. droidkungfu: Utilizes root exploits to gain complete control over the device. It can hide its presence and download additional components to expand its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Infector: Malware that attaches itself to executable files and spreads to other executable files.\n",
    "\n",
    "1. commplat: Targets and infects executable files, modifying them to include the malware's code. It can spread across networks and attached storage devices, embedding itself in other executables.\n",
    "2. leech: Infects various file types, especially executable files. It often appends its code to the host file and can corrupt files, making them unusable while ensuring its own propagation.\n",
    "3. tachi: Known for its stealthy infection techniques, it embeds malicious code into executable files without altering the file size significantly. It aims to remain undetected while spreading to other files.\n",
    "4. gudex: Primarily targets system and application files, integrating its code into the host files. This malware can lead to performance degradation and system instability as it spreads.\n",
    "5. aqplay: Employs sophisticated infection mechanisms to integrate its malicious code into executable files. It can evade detection by antivirus programs and spread through various vectors, including network shares and removable media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potentially Unwanted Applications (PUA): Software that may not be malicious but can negatively impact user experience, often installed without user consent.\n",
    "\n",
    "1. apptrack: Apptrack software is often used for tracking and monitoring user activities without explicit consent. It can collect data on app usage, browsing history, and user preferences, which can then be used for targeted advertising or sold to third parties.\n",
    "2. cauly: Cauly typically embeds itself in mobile applications and displays intrusive ads. It may collect user information and track browsing habits to serve personalized advertisements. The ads can degrade user experience and lead to unwanted data usage.\n",
    "3. secapk: Secapk is known for bundling with legitimate applications and performing actions like displaying ads, changing browser settings, and collecting data without user consent. It can be difficult to remove and may come with additional unwanted software.\n",
    "4. umpay: Umpay programs often integrate into mobile apps and facilitate in-app purchases or subscriptions without clear user consent. They might collect payment information and other personal data, leading to potential privacy concerns and unexpected charges.\n",
    "5. wiyun: Wiyun software is often associated with mobile advertising networks. It can display pop-up ads, modify browser settings, and collect user data for marketing purposes. The ads can be persistent and annoying, reducing the usability of infected devices.\n",
    "6. youmi: Youmi adware is known for aggressive advertising tactics. It displays numerous ads, collects user data, and can modify device settings to ensure its persistence. It is often bundled with other software, making it hard to detect and remove.\n",
    "7. utchi: Utchi typically comes bundled with freeware or shareware. It can display ads, redirect search queries, and collect personal information. The software may also introduce vulnerabilities by downloading additional unwanted applications.\n",
    "8. scamapp:  Scamapp refers to applications that deceive users into downloading them under false pretenses, such as offering fake security scans or system optimizations. Once installed, they can display misleading alerts, prompt for payment to fix non-existent issues, and collect personal information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ransomware: Malware that encrypts a user's data and demands a ransom to restore access.\n",
    "\n",
    "1. masnu: Masnu ransomware typically encrypts files on the infected device and demands a ransom in cryptocurrency for decryption. It may spread through malicious email attachments, exploit kits, or drive-by downloads.\n",
    "2. congur: Congur ransomware is known for locking the device screen rather than encrypting files. Victims are shown a ransom note demanding payment to unlock the screen. It often spreads through malicious apps or links.\n",
    "3. fusob: Fusob targets primarily Android devices. It pretends to be a fake system update or security software. Once installed, it locks the device and demands a ransom, usually in Bitcoin, to unlock it.\n",
    "4. jisut: Jisut ransomware locks the device screen with a ransom note in Chinese. It often spreads through malicious apps and typically demands payment through Chinese payment platforms. It may also display threatening messages to scare the victim into paying.\n",
    "5. koler: Koler ransomware targets Android devices. It often masquerades as a pornographic app or video player. Once installed, it locks the device and displays a fake law enforcement warning demanding a ransom to avoid prosecution.\n",
    "6. lockscreen: Lockscreen ransomware, as the name suggests, locks the user's screen and demands a ransom to unlock it. This family does not encrypt files but can make the device unusable by blocking access to the home screen and other functions.\n",
    "7. slocker: Slocker ransomware is one of the first Android ransomware families to use encryption. It encrypts files on the device and displays a ransom note demanding payment for the decryption key. It spreads through malicious apps and websites.\n",
    "8. smsspy: Smsspy ransomware not only locks the device or encrypts files but also has spyware capabilities. It can intercept SMS messages, steal contact information, and send messages to premium-rate numbers, increasing the victim's phone bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riskware: Legitimate software that poses a security risk due to its functionalities, which can be exploited\n",
    "\n",
    "1. skymobi: Collects user data and serves intrusive ads.\n",
    "2. anydown: Facilitates unauthorized downloads.\n",
    "3. badpac: Contains potentially harmful code or scripts.\n",
    "4. deng: Modifies device settings and installs unwanted apps.\n",
    "5. dnotua: Acts as a downloader for other malicious apps\n",
    "6. jiagu: Provides app obfuscation, often used by malicious apps to avoid detection\n",
    "7. metasploit: Exploit framework used for penetration testing, can be misused by attackers\n",
    "8. mobileplay: Engages in unauthorized transactions or payment services\n",
    "9. remotecode: Allows remote control of the device, potentially for malicious purposes\n",
    "10. revmob: Aggressive advertising network, collects user data\n",
    "11. secneo: Security application, can be used for legitimate or malicious purposes\n",
    "12. smspay: Sends SMS messages to premium-rate numbers without user consent\n",
    "13. smsreg: Registers users for premium services via SMS without consent\n",
    "14. talkw: Engages in unsolicited telemarketing calls or messages\n",
    "15. tencentprotect: Security app, may be used for both legitimate and malicious purposes\n",
    "16. tordow: Provides remote access and control, potentially for malicious use\n",
    "17. triada: Highly sophisticated malware with root capabilities, injects into system processes.\n",
    "18. wapron: Engages in unauthorized WAP billing\n",
    "19. nqshield: Security app, can be used for both protection and exploitation\n",
    "20. kingroot: Rooting tool that can be used to gain unauthorized access\n",
    "21. wificrack: Cracks Wi-Fi passwords, leading to unauthorized network access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scareware:  Malware designed to scare users into buying unnecessary software or services.\n",
    "\n",
    "1. avpass: Displays fake virus alerts to scare users into buying unnecessary software\n",
    "2. mobwin: Pop-up ads claiming the device is infected, urging users to download 'fixes\n",
    "3. fakeapp: Mimics legitimate apps to trick users into installing malicious software\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero - Day: Exploits targeting previously unknown vulnerabilities that have not been patched yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trojans\n",
    "\n",
    "1. Trojan: Malware disguised as legitimate software, which can execute harmful activities\n",
    "2. Trojan - Banker: Malware designed to steal banking information\n",
    "3. Trojan - Dropper: Malware that installs other malicious software onto the infected device\n",
    "4. Trojan - SMS: Malware that uses SMS services to send messages to premium numbers\n",
    "5. Trojan - SPY: Malware designed to spy on the user, collecting personal information and activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Category: Malware that doesn't fit into the predefined categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSVs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ad_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Android Malware/Dynamic/AndMal2020-Dynamic-BeforeAndAfterReboot/Adware_after_reboot_Cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(list(ad_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def create_img(rec, path, size=(10, 10)):\n",
    "    dt = rec.drop(['Category', 'Family'])\n",
    "    dt = pd.to_numeric(dt, errors='coerce').fillna(0)\n",
    "    data_array = dt.values.reshape(1, -1)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(data_array, cmap='viridis', aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main(df, base_dir, size=(10, 10)):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for idx, rec in df.iterrows():\n",
    "        cat = rec['Category']\n",
    "        fam = rec['Family']\n",
    "        fam_dir = os.path.join(base_dir, fam)\n",
    "        os.makedirs(fam_dir, exist_ok=True)\n",
    "        f_name= f\"{cat}_{fam}_{idx}.png\"\n",
    "        path = os.path.join(fam_dir, f_name)\n",
    "        create_img(rec, path, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dir = 'adware_family'\n",
    "main(ad_df, dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def metadata(df, base_dir):\n",
    "    j_dt = {}\n",
    "    for idx, rec in df.iterrows():\n",
    "        cat = rec['Category']\n",
    "        fam = rec['Family']\n",
    "        f_name = f\"{cat}_{fam}_{idx}.png\"\n",
    "\n",
    "        j_dt[f_name] = {\n",
    "            \"category\": cat,\n",
    "            \"family\": fam,\n",
    "            \"index\": idx\n",
    "        }\n",
    "\n",
    "    jf_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    with open(jf_path, 'w') as j_file:\n",
    "        json.dump(j_dt, j_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metadata(ad_df, \"/content/adware_family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Android Malware/Dynamic/AndMal2020-Dynamic-BeforeAndAfterReboot/Adware_after_reboot_Cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia Sets Visualization (as per the Paper)\n",
    "\n",
    "Ref Link: https://www.sciencedirect.com/science/article/pii/S0378475423004937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features = list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    G.add_node(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#edges can be added only by some random relationship\n",
    "import random\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(i + 1, len(features)):\n",
    "        if random.random() < 0.1:  # Randomly add some edges\n",
    "            G.add_edge(features[i], features[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fractal_formula(degree_cent, close_cent, betw_cent, eig_cent):\n",
    "    return lambda z: degree_cent*z + close_cent*z**5 + betw_cent*z**3 + eig_cent*z**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "node = random.choice(list(G.nodes))\n",
    "fractal_formula = generate_fractal_formula(degree_centrality[node], closeness_centrality[node], betweenness_centrality[node], eigenvector_centrality[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def visualize_fractal(fractal_formula, iterations=100, x_min=-2, x_max=2, y_min=-2, y_max=2, img_size=1000):\n",
    "    x = np.linspace(x_min, x_max, img_size)\n",
    "    y = np.linspace(y_min, y_max, img_size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = X + 1j*Y\n",
    "    C = np.zeros_like(Z, dtype=int)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        Z = fractal_formula(Z)\n",
    "        C[np.abs(Z) > 2] = i\n",
    "\n",
    "    plt.imshow(C, cmap='inferno', extent=(x_min, x_max, y_min, y_max))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "visualize_fractal(fractal_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Visualization using Dimensionality Reduction Techniques\n",
    "\n",
    "Ref Link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df = data.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scatterplot_matrix(data, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pd.plotting.scatter_matrix(pd.DataFrame(data), alpha=0.2, figsize=(15, 10))\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC 1', 'PC 2', 'PC 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scatterplot_matrix(pca_result, 'PCA Scatterplot Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['PC 1'], pca_df['PC 2'], c='blue', marker='o')\n",
    "plt.title('PCA Labeled Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_mlhl(data, output_dims=3, n_iter=100, learning_rate=0.2872, p=0.4852):\n",
    "    pca = PCA(n_components=output_dims)\n",
    "    result = pca.fit_transform(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mlhl_result = apply_mlhl(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mlhl_df = pd.DataFrame(data=mlhl_result, columns=['MLHL 1', 'MLHL 2', 'MLHL 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scatterplot_matrix(data, columns, title):\n",
    "    pd.plotting.scatter_matrix(pd.DataFrame(data, columns=columns), alpha=0.2, figsize=(15, 10))\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scatterplot_matrix(mlhl_result, ['MLHL 1', 'MLHL 2', 'MLHL 3'], 'MLHL Scatterplot Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(mlhl_df['MLHL 1'], mlhl_df['MLHL 2'], c='red', marker='o')\n",
    "plt.title('MLHL Labeled Components')\n",
    "plt.xlabel('MLHL Component 1')\n",
    "plt.ylabel('MLHL Component 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMLHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_cmlhl(data, output_dims=3, n_iter=100, learning_rate=0.0406, p=1.92, tau=0.44056):\n",
    "    pca = PCA(n_components=output_dims)\n",
    "    result = pca.fit_transform(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cmlhl_result = apply_cmlhl(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cmlhl_df = pd.DataFrame(data=cmlhl_result, columns=['CMLHL 1', 'CMLHL 2', 'CMLHL 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scatterplot_matrix(cmlhl_result, ['CMLHL 1', 'CMLHL 2', 'CMLHL 3'], 'CMLHL Scatterplot Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(cmlhl_df['CMLHL 1'], cmlhl_df['CMLHL 2'], c='green', marker='o')\n",
    "plt.title('CMLHL Labeled Components')\n",
    "plt.xlabel('CMLHL Component 1')\n",
    "plt.ylabel('CMLHL Component 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Each Records (Incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features = data.drop(['Hash', 'Category', 'Family'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scatterplot_pca_matrix(data, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pd.plotting.scatter_matrix(pd.DataFrame(data), alpha=0.2, figsize=(15, 10))\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_mlhl(data, output_dims=3, n_iter=100, learning_rate=0.2872, p=0.4852):\n",
    "    pca = PCA(n_components=output_dims)\n",
    "    result = pca.fit_transform(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_cmlhl(data, output_dims=3, n_iter=100, learning_rate=0.0406, p=1.92, tau=0.44056):\n",
    "    pca = PCA(n_components=output_dims)\n",
    "    result = pca.fit_transform(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scatterplot_matrix(data, columns, title):\n",
    "    pd.plotting.scatter_matrix(pd.DataFrame(data, columns=columns), alpha=0.2, figsize=(15, 10))\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, record in enumerate(features):\n",
    "    pca_result = pca.fit_transform(features)\n",
    "\n",
    "    #pp_mlhl = apply_mlhl(scaled_data)\n",
    "\n",
    "    #pp_cmlhl = apply_cmlhl(scaled_data)\n",
    "\n",
    "\n",
    "\n",
    "    scatterplot_pca_matrix(pca_result, f'PCA Scatterplot Matrix - Record {i}')\n",
    "\n",
    "\n",
    "    #scatterplot_matrix(pp_mlhl, ['MLHL 1', 'MLHL 2', 'MLHL 3'], f'MLHL Scatterplot Matrix - Record {i}')\n",
    "\n",
    "\n",
    "    #scatterplot_matrix(pp_cmlhl, ['CMLHL 1', 'CMLHL 2', 'CMLHL 3'], f'CMLHL Scatterplot Matrix - Record {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space-filling curves (SFCs) Visualization [Hilbert, Gray-code, and Z-order] (incomplete)\n",
    "\n",
    "Ref Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features = data.drop(['Hash', 'Category', 'Family'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilbert Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install hilbertcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from hilbertcurve.hilbertcurve import HilbertCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "p = 2\n",
    "n = int(np.sqrt(len(features[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hilbert_curve = HilbertCurve(p, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scaled_data = (scaled_data - np.min(scaled_data)) / (np.max(scaled_data) - np.min(scaled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image = np.zeros((n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i, record in enumerate(scaled_data):\n",
    "    distances = [int(val * 10) for val in record]  # Adjust the scale factor as needed\n",
    "    points = hilbert_curve.points_from_distances(distances)\n",
    "    for point in points:\n",
    "        image[point[0], point[1]] = 1\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Record {i + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = data.drop(columns=['Hash', 'Category', 'Family'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hilbert_curve(n):\n",
    "    def hilbert(x, y, xi, xj, yi, yj, n):\n",
    "        if n == 0:\n",
    "            points.append((x + (xi + yi) // 2, y + (xj + yj) // 2))\n",
    "        else:\n",
    "            hilbert(x, y, yi // 2, yj // 2, xi // 2, xj // 2, n - 1)\n",
    "            hilbert(x + xi // 2, y + xj // 2, xi // 2, xj // 2, yi // 2, yj // 2, n - 1)\n",
    "            hilbert(x + xi // 2 + yi // 2, y + xj // 2 + yj // 2, xi // 2, xj // 2, yi // 2, yj // 2, n - 1)\n",
    "            hilbert(x + xi // 2 + yi, y + xj // 2 + yj, -yi // 2, -yj // 2, -xi // 2, -xj // 2, n - 1)\n",
    "\n",
    "    points = []\n",
    "    hilbert(0, 0, n, 0, 0, n, int(np.log2(n)))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def map_features_to_h_curve(features, img_size):\n",
    "    h_curve_points = hilbert_curve(img_size)\n",
    "    img = np.zeros((img_size, img_size), dtype=np.float32)\n",
    "\n",
    "    for idx, (x, y) in enumerate(h_curve_points):\n",
    "        if idx < len(features):\n",
    "            img[x, y] = features[idx]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "\n",
    "output_dir = 'h_curve_images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for idx, row in enumerate(data_normalized):\n",
    "    features = row\n",
    "    img = map_features_to_h_curve(features, img_size)\n",
    "\n",
    "    plt.imsave(f'{output_dir}/h_curve_{idx}.png', img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractal Visualizations\n",
    "\n",
    "Ref Link: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = data.drop(['Hash', 'Category', 'Family'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "normalized_data = (df - df.min()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def julia_fractal(c, width, height, x_range, y_range, max_iter):\n",
    "    x = np.linspace(x_range[0], x_range[1], width)\n",
    "    y = np.linspace(y_range[0], y_range[1], height)\n",
    "    Z = x[:, np.newaxis] + 1j * y[np.newaxis, :]\n",
    "    C = np.full_like(Z, c)\n",
    "    img = np.zeros(Z.shape, dtype=int)\n",
    "    mask = np.ones(Z.shape, dtype=bool)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        Z[mask] = Z[mask] ** 2 + C[mask]\n",
    "        mask, old_mask = abs(Z) < 2, mask\n",
    "        img += mask & ~old_mask\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def mandelbrot_fractal(width, height, x_range, y_range, max_iter):\n",
    "    x = np.linspace(x_range[0], x_range[1], width)\n",
    "    y = np.linspace(y_range[0], y_range[1], height)\n",
    "    C = x[:, np.newaxis] + 1j * y[np.newaxis, :]\n",
    "    Z = np.zeros_like(C)\n",
    "    img = np.zeros(C.shape, dtype=int)\n",
    "    mask = np.ones(C.shape, dtype=bool)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        Z[mask] = Z[mask] ** 2 + C[mask]\n",
    "        mask, old_mask = abs(Z) < 2, mask\n",
    "        img += mask & ~old_mask\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def nova_fractal(p, R, c, width, height, x_range, y_range, max_iter):\n",
    "    x = np.linspace(x_range[0], x_range[1], width)\n",
    "    y = np.linspace(y_range[0], y_range[1], height)\n",
    "    Z = x[:, np.newaxis] + 1j * y[np.newaxis, :]\n",
    "    img = np.zeros(Z.shape, dtype=int)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        Z -= R * (Z ** p) / (p * Z ** (p - 1) + c)\n",
    "        img += abs(Z ** p - 1) < 1e-6\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "param_weights = np.random.uniform(-1, 1, (len(normalized_data.columns), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_params(row):\n",
    "    real_part = sum(row[feature] * weight for feature, weight in zip(normalized_data.columns, param_weights[:, 0]))\n",
    "    imag_part = sum(row[feature] * weight for feature, weight in zip(normalized_data.columns, param_weights[:, 1]))\n",
    "    a = sum(row[feature] * weight for feature, weight in zip(normalized_data.columns, param_weights[:, 2]))\n",
    "    b = sum(row[feature] * weight for feature, weight in zip(normalized_data.columns, param_weights[:, 3]))\n",
    "    return real_part, imag_part, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "params = normalized_data.apply(map_to_params, axis=1, result_type='expand')\n",
    "params.columns = ['real_part', 'imag_part', 'a', 'b']\n",
    "params['complex_constant'] = params['real_part'] + 1j * params['imag_part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df.join(params[['complex_constant', 'a', 'b']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_dirs = ['julia_images', 'mandelbrot_images', 'newton_images', 'nova_images']\n",
    "for output_dir in output_dirs:\n",
    "    os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "width, height = 800, 800\n",
    "x_range, y_range = [-1.5, 1.5], [-1.5, 1.5]\n",
    "max_iter = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    c = row['complex_constant']\n",
    "    a = row['a']\n",
    "    b = row['b']\n",
    "\n",
    "    julia_img = julia_fractal(c, width, height, x_range, y_range, max_iter)\n",
    "    mandelbrot_img = mandelbrot_fractal(width, height, x_range, y_range, max_iter)\n",
    "    newton_img = newton_fractal(a, b, width, height, x_range, y_range, max_iter)\n",
    "    nova_img = nova_fractal(3, 0.5, c, width, height, x_range, y_range, max_iter)\n",
    "\n",
    "    plt.imsave(f'julia_images/julia_{idx}.png', julia_img, cmap='inferno')\n",
    "    plt.imsave(f'mandelbrot_images/mandelbrot_{idx}.png', mandelbrot_img, cmap='inferno')\n",
    "    plt.imsave(f'newton_images/newton_{idx}.png', newton_img, cmap='inferno')\n",
    "    plt.imsave(f'nova_images/nova_{idx}.png', nova_img, cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gray Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features = data.columns[:-3]\n",
    "df = data[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "normalized_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "desired_image_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = 'grayscale_images_2'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for idx, record in enumerate(normalized_data):\n",
    "    image_array = np.zeros((desired_image_size, desired_image_size))\n",
    "    flat_image = image_array.flatten()\n",
    "    flat_image[:len(record)] = record\n",
    "    image_array = flat_image.reshape((desired_image_size, desired_image_size))\n",
    "    image_array = image_array.astype(np.uint8)  # Convert to uint8 type\n",
    "\n",
    "    # Save as image\n",
    "    plt.imsave(f'{output_dir}/image_{idx}.png', image_array, cmap='gray')\n",
    "\n",
    "print(\"Images saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity Based Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_img(rec, path, size=(5.12, 5.12), dpi=100):\n",
    "    dt = rec.drop(['Hash','Category', 'Family'])\n",
    "    dt = pd.to_numeric(dt, errors='coerce').fillna(0)\n",
    "    data_array = dt.values.reshape(1, -1)\n",
    "    plt.figure(figsize=size, dpi=dpi)\n",
    "    plt.imshow(data_array, cmap='viridis', aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main(df):\n",
    "    output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for idx, rec in df.iterrows():\n",
    "        cat = rec['Category']\n",
    "        fam = rec['Family']\n",
    "        f_name = f\"{cat}_{fam}_{idx}.png\"\n",
    "        path = os.path.join(output_dir, f_name)\n",
    "        create_img(rec, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def metadata(df, metadata_dict):\n",
    "    for idx, rec in df.iterrows():\n",
    "        cat = rec['Category']\n",
    "        fam = rec['Family']\n",
    "        f_name = f\"{cat}_{fam}_{idx}.png\"\n",
    "        metadata_dict[f_name] = {\n",
    "            \"category\": cat,\n",
    "            \"family\": fam,\n",
    "            \"index\": idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "zip_file_path = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/AndMal2020-Dynamic-BeforeAndAfterReboot.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "extract_dir = '/content/extracted_data'\n",
    "os.makedirs(extract_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files_dir = \"/content/extracted_data/AndMal2020-dynamic-BeforeAndAfterReboot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['Adware', 'Backdoor', 'FileInfector', 'No_Category', 'PUA', 'Ransomware', 'Riskware', 'Scareware', 'Trojan_Banker', 'Trojan_Dropper', 'Trojan_SMS', 'Trojan_Spy', 'Trojan', 'Zero_Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "  before_dataset = pd.read_csv(os.path.join(files_dir, f\"{cat}_before_reboot_Cat.csv\"))\n",
    "  after_dataset = pd.read_csv(os.path.join(files_dir, f\"{cat}_after_reboot_Cat.csv\"))\n",
    "  concatenated_df = pd.concat([before_dataset, after_dataset])\n",
    "  concatenated_df.reset_index(drop=True, inplace=True)\n",
    "  concatenated_df.to_csv(os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Dataset\", f\"{cat}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Concatenated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "actual_dataset_files = \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(actual_dataset_files) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metadata_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metadata_file_path = os.path.join(extract_dir, \"metadata.json\")\n",
    "with open(metadata_file_path, 'w') as j_file:\n",
    "    json.dump(metadata_dict, j_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(files_dir, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Processing {csv_file}\")\n",
    "    main(df)\n",
    "    metadata(df, metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = '/content/extracted_data/AndMal2020-dynamic-BeforeAndAfterReboot/Adware_after_reboot_Cat.csv'\n",
    "output_file = '/content/output.csv'\n",
    "num_records = 20\n",
    "with open(input_file, 'r', newline='') as infile, \\\n",
    "     open(output_file, 'w', newline='') as outfile:\n",
    "\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for index, row in enumerate(reader):\n",
    "        if index < num_records:\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/output.csv\")\n",
    "main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files_dir = \"/content/extracted_data/AndMal2020-dynamic-BeforeAndAfterReboot\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(files_dir) if f.endswith('.csv')]\n",
    "csv_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def csv_shape_info(csv_file):\n",
    "  df = pd.read_csv(csv_file)\n",
    "  shape = df.shape\n",
    "  return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "shape_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(actual_dataset_files, csv_file)\n",
    "    shape = csv_shape_info(file_path)\n",
    "    shape_info.append((csv_file, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/csv_shapes.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for csv_file, shape in shape_info:\n",
    "        f.write(f\"{csv_file}: {shape[0]} rows\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscaling Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_grayscale(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_array = np.array(img)\n",
    "\n",
    "            gray_array = 0.228 * img_array[:, :, 0] + 0.587 * img_array[:, :, 1] + 0.114 * img_array[:, :, 2]\n",
    "            gray_img = Image.fromarray(gray_array.astype(np.uint8))\n",
    "\n",
    "            gray_img_path = os.path.join(output_dir, img_name)\n",
    "            gray_img.save(gray_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_img_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images'\n",
    "output_img_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Grayscaled_Images'\n",
    "convert_to_grayscale(input_img_dir, output_img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##Directory Check\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def count_png_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if fnmatch.fnmatch(file, '*.png'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "directory_path = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Data_Augmentation'\n",
    "num_png_files = count_png_files(directory_path)\n",
    "print(num_png_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_save(input_dir, output_dir):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  index = 1\n",
    "  for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path)\n",
    "          base_name = os.path.splitext(img_name)[0]\n",
    "          for angle in [0, 90, 180, 270]:\n",
    "              rotated_img = img.rotate(angle)\n",
    "              rotated_img_path = os.path.join(output_dir, f'{base_name}_rotated_{angle}_{index}.png')\n",
    "              rotated_img.save(rotated_img_path)\n",
    "              index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rotate_save(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Data_Augmentation/Rotated Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def flip_save(input_dir, output_dir):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  index = 1\n",
    "\n",
    "  for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path)\n",
    "          base_name = os.path.splitext(img_name)[0]\n",
    "          flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "          flipped_img_path = os.path.join(output_dir, f'{base_name}_flipped_{index}.png')\n",
    "          flipped_img.save(flipped_img_path)\n",
    "          index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "flip_save(input_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", output_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Data_Augmentation/Flipped Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scale_save(input_dir, output_dir):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  index = 1\n",
    "  for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path)\n",
    "          base_name = os.path.splitext(img_name)[0]\n",
    "          scaled_img = img.resize((int(img.width * 1.5), int(img.height * 1.5)))\n",
    "          scaled_img_path = os.path.join(output_dir, f'{base_name}_scaled_{index}.png')\n",
    "          scaled_img.save(scaled_img_path)\n",
    "          index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scale_save(input_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", output_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Data_Augmentation/Scaled Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def augment_save(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    index = 1\n",
    "\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            for angle in [0, 90, 180, 270]:\n",
    "                rotated_img = img.rotate(angle)\n",
    "                rotated_img_path = os.path.join(output_dir, f'{base_name}_rotated_{angle}_{index}.png')\n",
    "                rotated_img.save(rotated_img_path)\n",
    "                index += 1\n",
    "            scaled_img = img.resize((int(img.width * 1.5), int(img.height * 1.5)))\n",
    "            scaled_img_path = os.path.join(output_dir, f'{base_name}_scaled_{index}.png')\n",
    "            scaled_img.save(scaled_img_path)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "augment_save(input_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Grayscaled_Images\", output_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Augmented_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "\n",
    "def resize_save(input_dir, output_dir, size=(512, 512)):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path)\n",
    "          img_array = np.array(img)\n",
    "          img_resized = transform.resize(img_array, size, anti_aliasing=True, mode='reflect')\n",
    "          img_resized_uint8 = (img_resized * 255).astype(np.uint8)\n",
    "          resized_img = Image.fromarray(img_resized_uint8)\n",
    "          resized_img_path = os.path.join(output_dir, img_name)\n",
    "          resized_img.save(resized_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "resize_save(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Resized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def normalize_image(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            img_array = np.array(img)\n",
    "            img_normalized = img_array / 255.0\n",
    "            normalized_img_path = os.path.join(output_dir, img_name)\n",
    "            Image.fromarray((img_normalized * 255).astype(np.uint8)).save(normalized_img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "normalize_image(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Filtering -> Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import filters\n",
    "\n",
    "def median_filtering(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            img_array = np.array(img)\n",
    "            img_filtered = filters.median(img_array)\n",
    "            filtered_img_path = os.path.join(output_dir, img_name)\n",
    "            Image.fromarray(img_filtered).save(filtered_img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "median_filtering(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Median_Filtered_Normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import filters\n",
    "\n",
    "def median_filtering_prevention(input_dir, output_dir, filter_size=3):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path).convert('L')\n",
    "            img_array = np.array(img)\n",
    "            footprint = np.ones((filter_size, filter_size), dtype=bool)\n",
    "            img_filtered = filters.median(img_array, footprint=footprint)\n",
    "            filtered_img_path = os.path.join(output_dir, img_name)\n",
    "            Image.fromarray(img_filtered).save(filtered_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "median_filtering_prevention(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Median_Filtered_Prevented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import filters\n",
    "\n",
    "def median_filtering_prevention(input_dir, output_dir, filter_size=3):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            img_array = np.array(img)\n",
    "            footprint = np.ones((filter_size, filter_size), dtype=bool)\n",
    "            img_filtered = filters.median(img_array, footprint=footprint)\n",
    "            filtered_img_path = os.path.join(output_dir, img_name)\n",
    "            Image.fromarray(img_filtered).save(filtered_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast and Edge Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import exposure, io\n",
    "\n",
    "def clahe_ce(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = io.imread(img_path)\n",
    "            img_clahe = exposure.equalize_adapthist(img)\n",
    "            clahe_img_path = os.path.join(output_dir, img_name)\n",
    "            io.imsave(clahe_img_path, img_clahe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "clahe_ce(input_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", output_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/CLAHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import exposure, io\n",
    "\n",
    "def clahe_ce_gs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = io.imread(img_path, as_gray=True)\n",
    "            img_float = img.astype(np.float64)\n",
    "            img_clahe = exposure.equalize_adapthist(img_float)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            clahe_img_path = os.path.join(output_dir, img_name)\n",
    "            io.imsave(clahe_img_path, img_clahe_uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "clahe_ce_gs(input_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", output_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/CLAHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def canny_ee_gs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            edges = cv2.Canny(img, 100, 200)\n",
    "            edge_img_path = os.path.join(output_dir, f'{img_name[:-4]}_edges.png')\n",
    "            cv2.imwrite(edge_img_path, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_ee_gs(input_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Images\", output_dir=\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Canny_Edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code - Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import transform, filters, exposure\n",
    "\n",
    "def preprocess_images(input_dir, output_dir, size=(512, 512), filter_size=3):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            img = Image.open(img_path) #resizing\n",
    "            img_resized = img.resize(size, resample=Image.LANCZOS)\n",
    "            img_array = np.array(img_resized) #normalizing\n",
    "            img_normalized = img_array / 255.0\n",
    "            img_filtered = filters.median(img_normalized, footprint= np.ones((filter_size, filter_size), dtype=bool)) #noise reduction median filtering\n",
    "            img_clahe = exposure.equalize_adapthist(img_filtered) #clahe\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            processed_img = Image.fromarray(img_clahe_uint8)\n",
    "            processed_img_name = f\"{os.path.splitext(img_name)[0]}__preprocessed.png\"\n",
    "            processed_img_path = os.path.join(output_dir, processed_img_name)\n",
    "            processed_img.save(processed_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Grayscaled_Images\", \"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation based Fractal Texture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from skimage import transform, filters, exposure\n",
    "import cv2\n",
    "\n",
    "class SFTA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_sfta_features(self, image):\n",
    "        thresholds = self.multi_level_otsu_thresholding(image)\n",
    "        sfta_vector = []\n",
    "        for i in range(len(thresholds) - 1):\n",
    "            binary_image = self.two_threshold_binary_decomposition(image, thresholds[i], thresholds[i + 1])\n",
    "            borders = self.find_borders(binary_image)\n",
    "            fractal_dimension = self.box_counting(borders)\n",
    "            mean_gray_level = np.mean(image[binary_image == 1])\n",
    "            pixel_count = np.sum(binary_image)\n",
    "            sfta_vector.extend([fractal_dimension, mean_gray_level, pixel_count])\n",
    "        return sfta_vector\n",
    "\n",
    "    def multi_level_otsu_thresholding(self, image):\n",
    "        thresholds = filters.threshold_multiotsu(image, classes=3)\n",
    "        return thresholds\n",
    "\n",
    "    def two_threshold_binary_decomposition(self, image, tlow, tup):\n",
    "        binary_image = np.zeros_like(image, dtype=np.uint8)\n",
    "        binary_image[(image > tlow) & (image <= tup)] = 1\n",
    "        return binary_image\n",
    "\n",
    "    def find_borders(self, binary_image):\n",
    "        borders = np.zeros_like(binary_image, dtype=np.uint8)\n",
    "        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(borders, contours, -1, 1, 1)\n",
    "        return borders\n",
    "\n",
    "    def box_counting(self, borders):\n",
    "        box_sizes = [2, 4, 8, 16, 32, 64]\n",
    "        counts = []\n",
    "        for size in box_sizes:\n",
    "            count = np.sum(borders[::size, ::size])\n",
    "            counts.append(count)\n",
    "        counts = np.array(counts)\n",
    "        counts = counts[counts > 0]\n",
    "        if counts.size > 0:\n",
    "            coefficients = np.polyfit(np.log(box_sizes[:counts.size]), np.log(counts), 1)\n",
    "            return -coefficients[0]\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import transform, filters, exposure\n",
    "\n",
    "def sfta_from_images(input_dir):\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            sfta = SFTA()\n",
    "            sfta_features = sfta.extract_sfta_features(img_clahe_uint8)\n",
    "            print(f\"Image: {img_name}, SFTA Features: {sfta_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sfta_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import feature, io\n",
    "\n",
    "def lbp(image):\n",
    "    radius = 3\n",
    "    n_points = radius * 8\n",
    "    lbp = feature.local_binary_pattern(image, n_points, radius, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    lbp_hist = lbp_hist.astype(np.float32)\n",
    "    lbp_hist /= (lbp_hist.sum() + 1e-8)\n",
    "    return lbp_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def lbp_from_images(input_dir):\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            lbp_features = lbp(img_clahe_uint8)\n",
    "            print(f\"Image: {img_name}, LBP Features: {lbp_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lbp_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haralick Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mahotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mahotas as mt\n",
    "\n",
    "def haralick(image):\n",
    "    textures = mt.features.haralick(image)\n",
    "    haralick_features = textures.mean(axis=0)\n",
    "    return haralick_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def haralick_from_images(input_dir):\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            haralick_features = haralick(img_clahe_uint8)\n",
    "            print(f\"Image: {img_name}, Haralick Features: {haralick_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "haralick_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "def gabor(image):\n",
    "    frequencies = [0.1, 0.5, 1.0]\n",
    "    thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    kernels = []\n",
    "    for theta in thetas:\n",
    "        for frequency in frequencies:\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta))\n",
    "            kernels.append(kernel)\n",
    "    gabor_responses = []\n",
    "    for kernel in kernels:\n",
    "        filtered = ndi.convolve(image, kernel, mode='reflect')\n",
    "        gabor_responses.append(filtered)\n",
    "    gabor_features = []\n",
    "    for response in gabor_responses:\n",
    "        energy = np.mean(response ** 2)\n",
    "        amplitude = np.mean(np.abs(response))\n",
    "        gabor_features.extend([energy, amplitude])\n",
    "\n",
    "    return gabor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def gabor_from_images(input_dir):\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            gabor_features = gabor(img_clahe_uint8)\n",
    "            print(f\"Image: {img_name}, Gabor Features: {gabor_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gabor_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tensorflow_features(image, model, preprocess_input, target_size=(224, 224)):\n",
    "        img_array = np.array(image)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features = model.predict(img_array)\n",
    "        return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def efficientnet_from_images(input_dir):\n",
    "    efficientnet_model = efficientnet.EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            processed_img = Image.fromarray(img_clahe_uint8)\n",
    "            efficientnet_features = extract_tensorflow_features(processed_image, efficientnet_model, efficientnet.preprocess_input, target_size=(224, 224))\n",
    "            print(f\"Image: {img_name}, EfficientNet Features: {efficientnet_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "efficientnet_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tensorflow_features(image, model, preprocess_input, target_size=(224, 224)):\n",
    "        img_array = np.array(image)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features = model.predict(img_array)\n",
    "        return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def inception_from_images(input_dir):\n",
    "    inception_model = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            processed_img = Image.fromarray(img_clahe_uint8)\n",
    "            inception_features = self.extract_tensorflow_features(processed_image, inception_model, inception_v3.preprocess_input, target_size=(299, 299))\n",
    "            print(f\"Image: {img_name}, Inception Features: {inception_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inception_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tensorflow_features(image, model, preprocess_input, target_size=(224, 224)):\n",
    "        img_array = np.array(image)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features = model.predict(img_array)\n",
    "        return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def vgg_from_images(input_dir):\n",
    "    vgg_model = vgg16.VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            processed_img = Image.fromarray(img_clahe_uint8)\n",
    "            vgg_features = extract_tensorflow_features(processed_image, vgg_model, vgg16.preprocess_input, target_size=(224, 224))\n",
    "            print(f\"Image: {img_name}, VGG Features: {vgg_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vgg_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tensorflow_features(image, model, preprocess_input, target_size=(224, 224)):\n",
    "        img_array = np.array(image)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features = model.predict(img_array)\n",
    "        return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_from_images(input_dir):\n",
    "    resnet_model = resnet.ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            processed_img = Image.open(img_path)\n",
    "            img_array = np.array(processed_img)\n",
    "            img_clahe = exposure.equalize_adapthist(img_array)\n",
    "            img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "            processed_img = Image.fromarray(img_clahe_uint8)\n",
    "            resnet_features = extract_tensorflow_features(processed_image, resnet_model, resnet.preprocess_input, target_size=(224, 224))\n",
    "            print(f\"Image: {img_name}, Resnet Features: {resnet_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "resnet_from_images(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Preprocessed_Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "concatenated_features = np.concatenate((sfta_features, lbp_features, haralick_features, gabor_features, efficientnet_features, inception_features, resnet_featuers, vgg_features))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
