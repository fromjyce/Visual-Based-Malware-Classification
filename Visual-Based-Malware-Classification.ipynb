{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Malware Detection Through Visual Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Selection using Chi Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def perform_chi_square_test(csv_file, output_file, target_column='Category', k=141):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X = df.drop(['Hash', 'Category', 'Family'], axis=1)\n",
    "    y = df[target_column]\n",
    "    chi2_scores, p_values = chi2(X, y)\n",
    "    feature_scores = pd.DataFrame({'Feature': X.columns, 'Chi2 Score': chi2_scores, 'P-value': p_values})\n",
    "    feature_scores.sort_values(by='Chi2 Score', ascending=False, inplace=True)\n",
    "    print(\"Chi-square test statistics:\")\n",
    "    print(feature_scores.head(k))\n",
    "    feature_scores.to_csv(output_file, sep='\\t', index=False)\n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/output.csv'\n",
    "output_file = '/content/chi2_results.txt'\n",
    "chi2_results = perform_chi_square_test(csv_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas matplotlib numpy pillow opencv-python scikit-image scipy #PC Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion from CSVs to RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Conversion:\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def create_img(self, rec, path, size=(5.12, 5.12), dpi=100):\n",
    "        dt = rec.drop(['Hash', 'Category', 'Family'])\n",
    "        dt = pd.to_numeric(dt, errors='coerce').fillna(0)\n",
    "        data_array = dt.values.reshape(1, -1)\n",
    "        plt.figure(figsize=size, dpi=dpi)\n",
    "        plt.imshow(data_array, cmap='viridis', aspect='auto')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    def process_csv(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for idx, rec in df.iterrows():\n",
    "            cat = rec['Category']\n",
    "            f_name = f\"{cat}_{idx}.png\"\n",
    "            path = os.path.join(self.output_dir, f_name)\n",
    "            self.create_img(rec, path)\n",
    "\n",
    "    def convert(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        for file_name in os.listdir(self.input_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(self.input_dir, file_name)\n",
    "                self.process_csv(csv_path)\n",
    "        return \"Conversion was successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Usage\n",
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024'\n",
    "output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images'\n",
    "conversion = Conversion(input_dir, output_dir)\n",
    "message = conversion.convert()\n",
    "print(message)\n",
    "\n",
    "\n",
    "##PC Usage\n",
    "##input_dir = ''\n",
    "##output_dir = ''\n",
    "##conversion = Conversion(input_dir, output_dir)\n",
    "##message = conversion.convert()\n",
    "##print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting RGB Images to Grayscaling Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_grayscale(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path).convert('RGB')\n",
    "          img_array = np.array(img)\n",
    "          gray_array = 0.228 * img_array[:, :, 0] + 0.587 * img_array[:, :, 1] + 0.114 * img_array[:, :, 2]\n",
    "          gray_img = Image.fromarray(gray_array.astype(np.uint8))\n",
    "          gray_img_path = os.path.join(output_dir, img_name)\n",
    "          gray_img.save(gray_img_path)\n",
    "    return \"Grayscale conversion was successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Usage\n",
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images'\n",
    "output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images/Grayscaled_Images/'\n",
    "gs_message = convert_to_grayscale(input_dir, output_dir)\n",
    "print(gs_message)\n",
    "\n",
    "\n",
    "##PC Usage\n",
    "##input_dir = ''\n",
    "##output_dir = ''\n",
    "##gs_message = convert_to_grayscale(input_dir, output_dir)\n",
    "##print(gs_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly Converting to Grayscaled Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Conversion:\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def create_img(self, rec, path, size=(5.12, 5.12), dpi=100):\n",
    "        dt = rec.drop(['Hash', 'Category', 'Family'])\n",
    "        dt = pd.to_numeric(dt, errors='coerce').fillna(0)\n",
    "        data_array = dt.values.reshape(1, -1)\n",
    "        plt.figure(figsize=size, dpi=dpi)\n",
    "        plt.imshow(data_array, cmap='gray', aspect='auto')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    def process_csv(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for idx, rec in df.iterrows():\n",
    "            cat = rec['Category']\n",
    "            f_name = f\"{cat}_{idx}.png\"\n",
    "            path = os.path.join(self.output_dir, f_name)\n",
    "            self.create_img(rec, path)\n",
    "\n",
    "    def convert(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        for file_name in os.listdir(self.input_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(self.input_dir, file_name)\n",
    "                self.process_csv(csv_path)\n",
    "        return \"Conversion was successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Usage\n",
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024'\n",
    "output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images'\n",
    "conversion = Conversion(input_dir, output_dir)\n",
    "message = conversion.convert()\n",
    "print(message)\n",
    "\n",
    "\n",
    "##PC Usage\n",
    "##input_dir = ''\n",
    "##output_dir = ''\n",
    "##conversion = Conversion(input_dir, output_dir)\n",
    "##message = conversion.convert()\n",
    "##print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre - Processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def augment_save(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    index = 1\n",
    "\n",
    "    for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path)\n",
    "          base_name = os.path.splitext(img_name)[0]\n",
    "          for angle in [0, 90, 180, 270]:\n",
    "            rotated_img = img.rotate(angle)\n",
    "            rotated_img_path = os.path.join(output_dir, f'{base_name}_rotated_{index}.png')\n",
    "            rotated_img.save(rotated_img_path)\n",
    "            index += 1\n",
    "          scaled_img = img.resize((int(img.width * 1.5), int(img.height * 1.5)))\n",
    "          scaled_img_path = os.path.join(output_dir, f'{base_name}_scaled_{index}.png')\n",
    "          scaled_img.save(scaled_img_path)\n",
    "          index += 1\n",
    "    return \"Image augmentation and saving was successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Usage\n",
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images/Grayscaled_Images'\n",
    "output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images/Augmented_Images'\n",
    "da_message = augment_save(input_dir, output_dir)\n",
    "print(da_message)\n",
    "\n",
    "\n",
    "##PC Usage\n",
    "##input_dir = ''\n",
    "##output_dir = ''\n",
    "##da_message = augment_save(input_dir, output_dir)\n",
    "##print(da_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mahotas\n",
    "\n",
    "!pip install tensorflow\n",
    "\n",
    "#!pip install pandas matplotlib numpy pillow opencv-python scikit-image scipy #PC Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import transform, filters, exposure, feature\n",
    "import cv2\n",
    "import mahotas as mt\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.filters import gabor_kernel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import inception_v3, efficientnet, vgg16, resnet\n",
    "\n",
    "class SFTA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_sfta_features(self, image):\n",
    "        thresholds = self.multi_level_otsu_thresholding(image)\n",
    "        sfta_vector = []\n",
    "        for i in range(len(thresholds) - 1):\n",
    "            binary_image = self.two_threshold_binary_decomposition(image, thresholds[i], thresholds[i + 1])\n",
    "            borders = self.find_borders(binary_image)\n",
    "            fractal_dimension = self.box_counting(borders)\n",
    "            mean_gray_level = np.mean(image[binary_image == 1])\n",
    "            pixel_count = np.sum(binary_image)\n",
    "            sfta_vector.extend([fractal_dimension, mean_gray_level, pixel_count])\n",
    "        return sfta_vector\n",
    "\n",
    "    def multi_level_otsu_thresholding(self, image):\n",
    "        thresholds = filters.threshold_multiotsu(image, classes=3)\n",
    "        return thresholds\n",
    "\n",
    "    def two_threshold_binary_decomposition(self, image, tlow, tup):\n",
    "        binary_image = np.zeros_like(image, dtype=np.uint8)\n",
    "        binary_image[(image > tlow) & (image <= tup)] = 1\n",
    "        return binary_image\n",
    "\n",
    "    def find_borders(self, binary_image):\n",
    "        borders = np.zeros_like(binary_image, dtype=np.uint8)\n",
    "        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(borders, contours, -1, 1, 1)\n",
    "        return borders\n",
    "\n",
    "    def box_counting(self, borders):\n",
    "        box_sizes = [2, 4, 8, 16, 32, 64]\n",
    "        counts = []\n",
    "        for size in box_sizes:\n",
    "            count = np.sum(borders[::size, ::size])\n",
    "            counts.append(count)\n",
    "        counts = np.array(counts)\n",
    "        counts = counts[counts > 0]\n",
    "        if counts.size > 0:\n",
    "            coefficients = np.polyfit(np.log(box_sizes[:counts.size]), np.log(counts), 1)\n",
    "            return -coefficients[0]\n",
    "        return 0\n",
    "\n",
    "class LBP:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_lbp_features(self, image):\n",
    "        radius = 3\n",
    "        n_points = radius * 8\n",
    "        lbp = feature.local_binary_pattern(image, n_points, radius, method='uniform')\n",
    "        lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "        lbp_hist = lbp_hist.astype(np.float32)\n",
    "        lbp_hist /= (lbp_hist.sum() + 1e-8)\n",
    "        return lbp_hist\n",
    "\n",
    "class Haralick:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_haralick_features(self, image):\n",
    "        textures = mt.features.haralick(image)\n",
    "        haralick_features = textures.mean(axis=0)\n",
    "        return haralick_features\n",
    "\n",
    "class Gabor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_gabor_features(self, image):\n",
    "        frequencies = [0.1, 0.5, 1.0]\n",
    "        thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "        kernels = []\n",
    "        for theta in thetas:\n",
    "            for frequency in frequencies:\n",
    "                kernel = np.real(gabor_kernel(frequency, theta=theta))\n",
    "                kernels.append(kernel)\n",
    "        gabor_responses = []\n",
    "        for kernel in kernels:\n",
    "            filtered = ndi.convolve(image, kernel, mode='reflect')\n",
    "            gabor_responses.append(filtered)\n",
    "        gabor_features = []\n",
    "        for response in gabor_responses:\n",
    "            energy = np.mean(response ** 2)\n",
    "            amplitude = np.mean(np.abs(response))\n",
    "            gabor_features.extend([energy, amplitude])\n",
    "        return gabor_features\n",
    "\n",
    "class FeatureExtraction:\n",
    "    def __init__(self):\n",
    "        self.sfta_extractor = SFTA()\n",
    "        self.lbp_extractor = LBP()\n",
    "        self.haralick_extractor = Haralick()\n",
    "        self.gabor_extractor = Gabor()\n",
    "        self.known_labels = [\n",
    "            \"Adware\", \"Backdoor\", \"FileInfector\", \"No_Category\", \"PUA\",\n",
    "            \"Ransomware\", \"Riskware\", \"Scareware\", \"Trojan_Banker\",\n",
    "            \"Trojan_Dropper\", \"Trojan_SMS\", \"Trojan_Spy\", \"Trojan\", \"Zero_Day\"\n",
    "        ]\n",
    "        self.inception_model = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "        self.efficientnet_model = efficientnet.EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "        self.vgg_model = vgg16.VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "        self.resnet_model = resnet.ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    def extract_tensorflow_features(self, image, model, preprocess_input, target_size=(224, 224)):\n",
    "        img_array = np.array(image)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features = model.predict(img_array)\n",
    "        return features.flatten()\n",
    "\n",
    "    def extract_features(self, preprocessed_image, processed_image):\n",
    "        sfta_features = self.sfta_extractor.extract_sfta_features(preprocessed_image)\n",
    "        lbp_features = self.lbp_extractor.extract_lbp_features(preprocessed_image)\n",
    "        haralick_features = self.haralick_extractor.extract_haralick_features(preprocessed_image)\n",
    "        gabor_features = self.gabor_extractor.extract_gabor_features(preprocessed_image)\n",
    "        inception_features = self.extract_tensorflow_features(processed_image, self.inception_model, inception_v3.preprocess_input, target_size=(299, 299))\n",
    "        efficientnet_features = self.extract_tensorflow_features(processed_image, self.efficientnet_model, efficientnet.preprocess_input, target_size=(224, 224))\n",
    "        vgg_features = self.extract_tensorflow_features(processed_image, self.vgg_model, vgg16.preprocess_input, target_size=(224, 224))\n",
    "        resnet_features = self.extract_tensorflow_features(processed_image, self.resnet_model, resnet.preprocess_input, target_size=(224, 224))\n",
    "        return sfta_features, lbp_features, haralick_features, gabor_features, inception_features, efficientnet_features, vgg_features, resnet_features\n",
    "\n",
    "    def extract_label_from_filename(self, filename):\n",
    "        for label in self.known_labels:\n",
    "            if label.replace('_', ' ').replace(' ', '_') in filename:\n",
    "                return label\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def get_features_and_label(self, image_path, preprocessed_image, processed_img):\n",
    "        filename = os.path.basename(image_path)\n",
    "        sfta_features, lbp_features, haralick_features, gabor_features, inception_features, efficientnet_features, vgg_features, resnet_features = self.extract_features(preprocessed_image, processed_img)\n",
    "        label = self.extract_label_from_filename(filename)\n",
    "        return sfta_features, lbp_features, haralick_features, gabor_features, inception_features, efficientnet_features, vgg_features, resnet_features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfta_features_list = []\n",
    "lbp_features_list = []\n",
    "haralick_features_list = []\n",
    "gabor_features_list = []\n",
    "inception_features_list = []\n",
    "efficientnet_features_list = []\n",
    "vgg_features_list = []\n",
    "resnet_features_list = []\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import transform, filters, exposure\n",
    "\n",
    "def preprocess_images(input_dir, output_dir, size=(512, 512), filter_size=3):\n",
    "    sfta_features_list = []\n",
    "    lbp_features_list = []\n",
    "    haralick_features_list = []\n",
    "    gabor_features_list = []\n",
    "    inception_features_list = []\n",
    "    efficientnet_features_list = []\n",
    "    vgg_features_list = []\n",
    "    resnet_features_list = []\n",
    "    labels_list = []\n",
    "    feature_extractor = FeatureExtraction()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(input_dir):\n",
    "      if img_name.endswith('.png'):\n",
    "          img_path = os.path.join(input_dir, img_name)\n",
    "          img = Image.open(img_path) #resizing\n",
    "          img_resized = img.resize(size, resample=Image.LANCZOS)\n",
    "          img_array = np.array(img_resized) #normalizing\n",
    "          img_normalized = img_array / 255.0\n",
    "          img_filtered = filters.median(img_normalized, footprint= np.ones((filter_size, filter_size), dtype=bool)) #noise reduction median filtering\n",
    "          img_clahe = exposure.equalize_adapthist(img_filtered) #clahe\n",
    "          img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "\n",
    "          processed_img = Image.fromarray(img_clahe_uint8)\n",
    "          processed_img_name = f\"{os.path.splitext(img_name)[0]}__preprocessed.png\"\n",
    "          processed_img_path = os.path.join(output_dir, processed_img_name)\n",
    "          #processed_img.save(processed_img_path)\n",
    "\n",
    "          sfta_features, lbp_features, haralick_features, gabor_features, inception_features, efficientnet_features, vgg_features, resnet_features, label = feature_extractor.get_features_and_label(processed_img_path, img_clahe_uint8)\n",
    "          sfta_features_list.append(sfta_features)\n",
    "          lbp_features_list.append(lbp_features)\n",
    "          haralick_features_list.append(haralick_features)\n",
    "          gabor_features_list.append(gabor_features)\n",
    "          inception_features_list.append(inception_features)\n",
    "          efficientnet_features_list.append(efficientnet_features)\n",
    "          vgg_features_list.append(vgg_features)\n",
    "          resnet_features_list.append(resnet_features)\n",
    "          labels_list.append(label)\n",
    "\n",
    "    return sfta_features_list, lbp_features_list, haralick_features_list, gabor_features_list, inception_features_list, efficientnet_features_list, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Usage\n",
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images/Augmented_Images'\n",
    "output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images/Preprocessed_Images'\n",
    "sfta_features_list, lbp_features_list, haralick_features_list, gabor_features_list, inception_features_list, efficientnet_features_list, labels = preprocess_images(input_dir, output_dir)\n",
    "\n",
    "\n",
    "##PC Usage\n",
    "##input_dir = ''\n",
    "##output_dir = ''\n",
    "##features_list, labels = preprocess_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the Features into CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import transform, filters, exposure\n",
    "\n",
    "def preprocess_images(input_dir, output_dir, output_csv, size=(512, 512), filter_size=3):\n",
    "    feature_extractor = FeatureExtraction()\n",
    "    fieldnames = ['SFTA', 'LBP', 'Haralick', 'Gabor', 'Inception', 'EfficientNet', 'VGG', 'Resnet', 'Label']\n",
    "    csv_exists = os.path.exists(output_csv)\n",
    "    with open(output_csv, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not csv_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for img_name in os.listdir(input_dir):\n",
    "            if img_name.endswith('.png'):\n",
    "                img_path = os.path.join(input_dir, img_name)\n",
    "                img = Image.open(img_path)\n",
    "                rgb_image = img.convert('RGB')\n",
    "                img_resized = img.resize(size, resample=Image.LANCZOS)\n",
    "                rgb_img_resized = rgb_image.resize(size, resample=Image.LANCZOS)\n",
    "                img_array = np.array(img_resized)\n",
    "                rgb_img_array = np.array(rgb_img_resized)\n",
    "                img_normalized = img_array / 255.0\n",
    "                rgb_img_normalized = rgb_img_array / 255.0\n",
    "                footprint = np.ones((filter_size, filter_size), dtype=bool)\n",
    "                footprint_rgb = np.ones((filter_size, filter_size, 3))\n",
    "                img_filtered = filters.median(img_normalized, footprint=footprint)\n",
    "                rgb_img_filtered = filters.median(rgb_img_normalized, footprint=footprint_rgb)\n",
    "                img_clahe = exposure.equalize_adapthist(img_filtered)\n",
    "                rgb_img_clahe = exposure.equalize_adapthist(rgb_img_filtered)\n",
    "                img_clahe_uint8 = (img_clahe * 255).astype(np.uint8)\n",
    "                rgb_img_clahe_uint8 = (rgb_img_clahe * 255).astype(np.uint8)\n",
    "                processed_img = Image.fromarray(img_clahe_uint8)\n",
    "                rgb_processed_img = Image.fromarray(rgb_img_clahe_uint8)\n",
    "                processed_img_name = f\"{os.path.splitext(img_name)[0]}__preprocessed.png\"\n",
    "                processed_img_path = os.path.join(output_dir, processed_img_name)\n",
    "                #processed_img.save(processed_img_path)\n",
    "\n",
    "                sfta_features, lbp_features, haralick_features, gabor_features, inception_features, efficientnet_features, vgg_features, resnet_features, label = feature_extractor.get_features_and_label(processed_img_path, img_clahe_uint8, rgb_processed_img)\n",
    "                writer.writerow({\n",
    "                    'SFTA': ','.join(map(str, sfta_features)),\n",
    "                    'LBP': ','.join(map(str, lbp_features)),\n",
    "                    'Haralick': ','.join(map(str, haralick_features)),\n",
    "                    'Gabor': ','.join(map(str, gabor_features)),\n",
    "                    'Inception': ','.join(map(str, inception_features)),\n",
    "                    'EfficientNet': ','.join(map(str, efficientnet_features)),\n",
    "                    'VGG': ','.join(map(str, vgg_features)),\n",
    "                    'Resnet': ','.join(map(str, resnet_features)),\n",
    "                    'Label': label\n",
    "                })\n",
    "\n",
    "    return \"Preprocessing and Generating CSV File were successfull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Test_Filtered_Converted_Images/Augmented_Images/Backdoor'\n",
    "output_dir = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Filtered_Converted_Images/Preprocessed_Images'\n",
    "output_csv = '/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Test_Filtered_Converted_Images/Test_Filtered_Features.csv'\n",
    "pe_message = preprocess_images(input_dir, output_dir, output_csv)\n",
    "\n",
    "print(pe_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/SRFP 2024/Test_Filtered_Converted_Images/Test_Filtered_Features.csv\")\n",
    "\n",
    "print(test.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
